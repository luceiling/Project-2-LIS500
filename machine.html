<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="keywords" content="intersectionality, implicit bias test, bias, implicit association test, equity">
        <meta name="author" content="HTML: Larkspur Domka; CSS: Lucy Mackintosh">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>LIS 500 Project 2 Resources</title>
        <link rel="stylesheet" href="stylepage.css">
    </head>
    <body>
        
        <!-- Top navigation bar -->
        <div class="topnav">
            <form action="index.html">
                <button type="submit" class="btn">Home</button>
            </form>
            <form action="about.html">
              <button type="submit" class="btn">About Us</button>
            </form>
            <form action="resources.html">
             <button type="submit" class="btn">Resources</button>
            </form>
            <form action="hero.html">
             <button type="submit" class="btn">Tech Heroes</button>
            </form>
            <form action="machine.html">
            <button type="submit" class="btn">Machine Learning</button>
            </form>
        </div>
        
        <!--about the model-->
        
        <h1 class="home">Project Purpose</h1>
        <h2 class="sectionTitle">What we did</h2>
        <h3>Step 1: Planning</h3>
        <p class="reflectBody">
            We had multiple Zoom meetings to plan out the process and ideas we had. We started ambitious, thinking we could do focus on training a machine to learn biomes or seasons, but we decided that, with how broad that category could be and how diverse each area shows its seasons, it may be too difficult to do. We also wanted to use mostly our own images in the model. Lark had some photos of Minnesota wildnerness, but we would be a bit beholden to the region and seasons he had photographed. <br><br>
            But through this line of thinking, because of Lark's art, we decided that it would be interesting to explore whether AI would be able to tell the difference between art and human. We extended this a bit to "not real human" and "real human" to be a little more objective in our model. 
        </p>
        <h3>Step 2: Compiling Images</h3>
        <p class="reflectBody">
            Lark here: with my BA in studio art, I have a moderately sized portfolio of work to pull images from. One of the visual art styles I specialised in for my degree was hyperrealism/magical realism--I took real objects and figures and added surreal or fantastical elements/qualities to them.<br><br>
            After reading Boulamwini's book, it had me thinking about a machine's capability to detect the presence of real humans. I wondered if it would be able to differentiate between non-photographic art objects depicting humans/human anatomy and images that might contain similar patterns of hue/saturation/value mapping vs images that contain real humans, including images of real humans that had been edited and stylized with touch-ups and color adjustments.<br><br>
            A  note on what I mean when I say "hue/saturation/value mapping" for those not as versed in color theory: every color, both in real life and virtual, has, at minimum, three core properties which are used to describe how it is generally perceived by the human eye. The three qualities are hue (red, yellow, green, and blue are hues), saturation (how intense or "pure" a color is vs. how close it is to white/gray/black), and value (how light or dark a color is).  You may have heard of or seen "HSV", which is an abbreviation for hue/saturation/value, and is a color model used in graphic design.  In order for computers to display different colors, there are a few code systems in existence that essentially calculate or designate the HSV of every pixel displayed on the monitor. Similar to how HTML is a language for web page structure, there are also computing "languages" of colors (hexidecimal, RGB, CMYK, etc.).  All the pixels of an image taken together create a map of color codes--I would hypothesize that teachable machines use some of this color information when learning from image data. Therein, a realistic human figure drawing could have similar shapes/patterns of color value data as a photo of a human, even if the drawing is in grayscale.  So if the algorithm is leaning on a specific color property more than others for determining whether an image contains a real human or not, that could impact its output confidence.<br><br>
            I chose images of figure drawings I did, and one hyperrealistic digital painting I made of a friend; only the figure is painted in that image, the background is the original photo.  I also added various more "professional" photos I took of landscapes and people, including photos of myself.  Some of these photos have minor touch ups, HSV adjustments done in editing, or are close-ups on particular human anatomy (hands, fingers, mouths, etc.).  My hope was that the algorithm would be able to then pick up on whether real/photographed human anatomy was present in any part of any given image.<br><br>
            Some shortcomings of the dataset were that I haven't done enough photoshoots of people to have a diverse portfolio of human/figure-based photography.  I had photographed myself, a couple of former housemates, and someone else I went to undergrad with, all of whom are white or otherwise lighter skinned.  Lucy added some stock photos of other humans for greater diversity.  In searching through my files, I also realized I have lost some of the higher quality images I had of my non-photographic artwork, and a number of my art pieces were damaged in a flood a few years back so trying to photograph them now unfortunately wouldn't have done much good.
        </p>
        <h3>Step 3: Training the Model</h3>
        <p class="reflectBody">
            
        </p>
        <h2 class="sectionTitle">Results</h2>
        <h2 class="sectionTitle">What we learned</h2>
    </body>
</html>
        
