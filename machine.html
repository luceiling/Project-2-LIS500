<!--Note: Lark did the images and art, Lucy did the coding and programming and website stuff, and Sara did the final write up. We all did the planning phases and added to the webpage an equal amount. -->
<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="keywords" content="intersectionality, implicit bias test, bias, implicit association test, equity">
        <meta name="author" content="HTML: Larkspur Domka; CSS: Lucy Mackintosh">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>LIS 500 Project 2 Resources</title>
        <link rel="stylesheet" href="stylepage.css">
    </head>
    <body>
        
        <!-- Top navigation bar -->
            <div class="topnav">
            <form action="index.html">
                <button type="submit" class="btn">Home</button>
            </form>
            <form action="about.html">
              <button type="submit" class="btn">About Us</button>
            </form>
            <form action="resources.html">
             <button type="submit" class="btn">Resources</button>
            </form>
            <form action="hero.html">
             <button type="submit" class="btn">Tech Heroes</button>
            </form>
            <form action="machine.html">
                <button type="submit" class="btn">Machine Learning</button>
            </form>
            <form action="indexml.html">
                <button type="submit" class="btn">Machine Learning Model</button>
            </form>
        </div>
        
        <!--about the model-->
  <div style="margin-left: 35px; margin-right: 35px; margin-bottom: 35px">     
        <h1 class="home">Our Machine Learning Model</h1>
        <h2 class="sectionTitle">What We Did</h2>
      
      <!--Planning-->
        <h3>Step 1: Planning</h3>
        <p class="reflectBody">
            We had multiple Zoom meetings to plan out the process and ideas we had. We started ambitious, thinking we could do focus on training a machine to learn biomes or seasons, but we decided that, with how broad that category could be and how diverse each area shows its seasons, it may be too difficult to do. We also wanted to use mostly our own images in the model. Lark had some photos of Minnesota wilderness, but we would be a bit beholden to the region and seasons he had photographed. <br><br>
            But through this line of thinking, because of Lark's art, we decided that it would be interesting to explore whether AI would be able to tell the difference between art and human. We extended this a bit to "not real human" and "real human" to be a little more objective in our model. 
        </p>
      
      <!--Samples/Images-->
        <h3>Step 2: Compiling Images</h3>
        <h4>Lark:</h4>
        <p class="reflectBody">
            With my BA in studio art, I have a moderately sized portfolio of work to pull images from. One of the visual art styles I specialized in for my degree was hyperrealism/magical realism--I took real objects and figures and added surreal or fantastical elements/qualities to them.<br><br>
            After reading Boulamwini's book, it had me thinking about a machine's capability to detect the presence of real humans. I wondered if it would be able to differentiate between non-photographic art objects depicting humans/human anatomy and images that might contain similar patterns of hue/saturation/value mapping vs images that contain real humans, including images of real humans that had been edited and stylized with touch-ups and color adjustments.<br><br>
            A  note on what I mean when I say "hue/saturation/value mapping" for those not as versed in color theory: every color, both in real life and virtual, has, at minimum, three core properties which are used to describe how it is generally perceived by the human eye. The three qualities are hue (red, yellow, green, and blue are hues), saturation (how intense or "pure" a color is vs. how close it is to white/gray/black), and value (how light or dark a color is).  You may have heard of or seen "HSV", which is an abbreviation for hue/saturation/value, and is a color model used in graphic design.  In order for computers to display different colors, there are a few code systems in existence that essentially calculate or designate the HSV of every pixel displayed on the monitor. Similar to how HTML is a language for web page structure, there are also computing "languages" of colors (hexadecimal, RGB, CMYK, etc.).  All the pixels of an image taken together create a map of color codes--I would hypothesize that teachable machines use some of this color information when learning from image data. Therein, a realistic human figure drawing could have similar shapes/patterns of color value data as a photo of a human, even if the drawing is in grayscale.  So if the algorithm is leaning on a specific color property more than others for determining whether an image contains a real human or not, that could impact its output confidence.<br><br>
            I chose images of figure drawings I did, and one hyperrealistic digital painting I made of a friend; only the figure is painted in that image, the background is the original photo.  I also added various more "professional" photos I took of landscapes and people, including photos of myself.  Some of these photos have minor touch-ups, HSV adjustments done in editing, or are close-ups on particular human anatomy (hands, fingers, mouths, etc.).  My hope was that the algorithm would be able to then pick up on whether real/photographed human anatomy was present in any part of any given image.<br><br>
            Some shortcomings of the dataset were that I haven't done enough photoshoots of people to have a diverse portfolio of human/figure-based photography.  I had photographed myself, a couple of former housemates, and someone else I went to undergrad with, all of whom are white or otherwise lighter-skinned.  Lucy added some stock photos of other humans for greater diversity.  In searching through my files, I also realized I have lost some of the higher quality images I had of my non-photographic artwork, and a number of my art pieces were damaged in a flood a few years back so trying to photograph them now unfortunately wouldn't have done much good.<br><br>
            Other things that likely cause some of the inaccuracies in our model include what I described above: hyperrealistic artworks will have ink/paint strokes that very closely mimic real HSV maps.  In fact, if I upload my hyperrealistic digital portrait painting into the model, the model has a high output confidence that the image contains a real human (inaccurate), even though the model was trained on this image specifically.  I tried testing another version of the same painting where the background is just a gradient instead of a photograph, and the machine still thinks it contains a real human with 99% confidence.  As was mentioned a few times throughout Boulamwini’s book, we don’t know what all cues the machine is using to make its determinations; we don’t know precisely how it is going from the training data to its new outputs.  In this case, it seems like the image’s background does not play heavily into the output decision.  So, having trained the algorithm using some hyperrealistic paintings/drawings, it seems we have exposed just a fraction of the fallibility inherent in machine learning.
            <br><br> <a href="https://github.com/luceiling/Project-2-LIS500/tree/be37f9cc61b5d0a42db33905c5999c1f19c85eb2/Images" target="_blank">Take a look at the images we used to train our model!</a>
        </p>
      
      <!--Programming the model, training the model-->
        <h3>Step 3: Training the Model and Programming the Model</h3>
        <h3>Training the Model</h3>
        <p class="reflectBody">
            We used <a href="https://teachablemachine.withgoogle.com/" target="_blank">Google's teachable machine</a> to analyze our images and create a model we could use to finally create <a href="https://teachablemachine.withgoogle.com/models/Q0Tl7D-bs/" target="_blank">our basic model</a>. The image selection process is discussed above.</p>
        <h3>Programming the Model</h3>
        <p class="reflectBody">We debated making the program specifically only use live image input, and created the program for that, but decided that it did not work for what we wanted. The main issue with the video feed was that, probably because of our lower sample base, there was too much variation of results even when there was little movement on camera.<br><br>The three of us agreed that we liked the image uploader on the basic Google model better for our preferences, so Lucy <a href="https://editor.p5js.org/luceiling/sketches/72J3BSUOr" target="_blank"> redid the code on p5js.org</a> to make uploading images an option. Thankfully, there are a ton of resources available to make this process a little easier, and Lucy was able to find a guide that walked through this process. While some issues still exist, like the dimensions of the images stretching, making some of them look a little weird and maybe impacting the results of the model (e.g., some of the images we used in the model only show up with 0.99 confidence), this still ended up working really well and having a more consistent confidence rating over the video feed version. In the video below, you can see that the model has an easier time telling what is not human than what is, as the confidence is, on average, lower for actual humans verses non-human images, but it is still pretty accurate. See the video below to see how it works, or try it out yourself!
        </p>
      
      <!--The model, our example, etc.-->
        <h2 class="sectionTitle">Results</h2>
            <div style="text-align: center;">
                <form action="indexml.html" target="_blank>">
                    <button type="submit" class="btn" id="iat-btn">Try Out Our Machine Learning Model</button>
                </form>
            </div>
      <h2 class="sectionTitle">Our Model in Action</h2>
        <center>
            <video width="700" height="" controls>
                <source src="LIS500_Machine_Learning_Showcase.mp4" type="video/mp4">
                Your browser does not support the video tag
            </video>
        </center>
      
      <!--reflection, conclusion, resources-->
        <h2 class="sectionTitle">Reflection</h2>
        <p class="reflectBody">
            Working on this project gave us a much greater understanding of Buolamwini’s book. Particularly, we gained more perspective on the complications with training an AI, how bias can negatively impact the results, and the issues of collecting a large dataset. The first aspect of Buolamwini’s book that came up while we worked was the question of consent. Buolamwini states, “Was the data obtained with consent? What were the working conditions and compensation for the workers who processed the data?” (Buolamwini 68). This was one of our chief concerns when we started the project. Where would we obtain our images? How would we ensure we obtained the images legally? We did not want to illegally obtain images, stealing images that the artists worked hard on. There was no chance of the artists getting compensation from our usage. We almost immediately decided to use our own images to avoid this issue, but this is not a viable solution when thousands or even millions of images are needed. 
            <br><br>
            Staying on the topic of consent, it played a major role in how we chose our topic. Lucy almost immediately inquired about the scope of the data because where to get the images was a major concern. We did not want to collect images without the consent of the people who took them/were featured in them. Buolamwini reflected on this struggle, saying, “Nevertheless, lawful use did not overcome the basic fact that I would be using images of people’s faces without their consent to do this research, unless I could somehow obtain the consent of the 1,270 individuals who would be included in the dataset” (113). While we settled on using our own images, what would we have done if we needed a bigger dataset? If we needed hundreds or thousands of images, we would have had to use images without consent, which felt wrong. Buolamwini’s (113) concerns were shot down, which is surprising. Our project has way less real-world implications than Buolamwini’s and we still considered this to be a major concern. 
            <br><br>
            Another concern that came up as we worked was the AI’s ability to recognize us as human. The AI failed to consistently recognize us as human, which can be incredibly concerning in a real-world context. Buolamwini (71) expresses two concerns with AI facial recognition; she discusses difficulty she had with joining a dating app because it used AI that did not recognize her face, and she talks about someone who was incarcerated based on a false facial recognition match. While we will not be using our AI in these kinds of real-world situations, these examples show the issue that our AI’s inability to recognize some faces as human can cause. Based on Buolamwini’s findings, we cannot use our AI in the real world. With our limited dataset, it can cause issues ranging from being unable to use certain apps to false arrests. What if something similar to our AI was used for a captcha? How many people would not be able to use the Internet adequately? 
            <br><br>
            This quote from Buolamwini (94) reflects why our results may not be perfect and why our dataset is limited, “This is not always the case, particularly when the experts do not reflect the rest of society.” We did not go into this project with any malice, but we have a limited dataset, based on the people, drawings, and environments we have access to. We do not represent society as a whole, so our dataset also does not represent society as a whole. The big difference between us and Buolamwini’s example is what we have access to. We used our own images, while the experts have access to millions of images. They should have access to a diverse range of data, but they do not include it when training AI models. Buolamwini also mentions that the government dataset had a slight majority of light-skinned males, with less than five percent of the dataset being dark-skinned females (94). This has been a concern during our project. We do not have a diverse set of humans for the AI to learn from. The AI is generally very accurate, but it would certainly be more accurate if we had a more diverse set of data. 
            <br><br>
            It should be noted that while it may go slightly against our ideas for full consent, we did use all of the images on our website, including our tech heroes, to train the AI. We did this for a few reasons. First, it tied back to the discussions of intersectionality that we had earlier this semester. Our limited dataset made it very difficult to properly implement intersectionality in our model, so doing this allowed us to train the AI with more intersectionality–specifically black women in this case. This also ties back to the article by Erica Joy. She discusses the lack of representation for women of color in tech. We did not want to lack this representation in our training set and repeat the troubling history of ignoring women of color. Joy (8) makes the problem of not valuing representation and intersectionality clear and that is not something we wanted to perpetuate, even if it meant sacrificing some of the consent we strived for.  
            <br><br>
            Ruha Benjamin’s (4) article also made some points that stuck with us while working on this project. She quotes Cathy O’Neil, pointing out that tech development needs to be handled from a socially conscious perspective, prioritizing equity and social good (Benjamin 4). Training our own AI made us realize how important these ideas are. While we did not have the monetary incentive most tech workers have, we do have a grade incentive. It is easier to use a very limited dataset, but we did our best to push for representation in our dataset because that is the right thing to do, even if it may not be the easiest or have the biggest grade incentive. We trained our AI with a lot of intention, which may be more difficult, but is much more rewarding.
            <br><br>
            When Buolamwini (107-108) was working on classification, she struggled with the bias that comes with naming the different classifications, finally settling on something more objective. She settled on the specific color of people’s skin, rather than race or ethnicity because it is “more specific and objective” (Buolamwini 108). This resonated with us as we decided how to do our own classifications. We settled on the classifications of real human vs. not real human because it is objective. Our project does not have as many complications as Buolamwini’s in this regard, but remaining objective with our classifications was still something we took away from her work. 
            <br><br>
            Ellen Pao’s (39) focus on the principle of “do no harm” was also important to us. Particularly, the idea of holding ourselves accountable was important. For example, if the AI does not work as intended, whose fault is that? Most likely ours since we are the ones training the AI. So, what can we do about that? We knew from the beginning that we needed to hold ourselves accountable for any issues that come up and make changes when problems arise. This is not easy, as it can be difficult to diagnose problems with AI, but it is our responsibility. We cannot do much about the lack of diversity within our team, but we can hold ourselves accountable for lack of diversity and representation and make changes when needed. 
            <br><br> 
            Buolamwini finishes off her novel with a poem titled “Still Uncompleted,” which resonates with what we learned. The people training AI, like we did during this project, need to view humanity as more than data. So many things need to be considered when working with AI. We considered so many issues while working on this project, and I doubt we would have made these considerations without reading Buolamwini’s book. Consent is certainly a tricky aspect of AI, but it needs to be at least considered. Most people would not be happy with their face being used to train AI, so it is concerning that their likeness can be used without consent. It is also concerning that people’s labor is being used to train AI without their work being compensated. There are concerns of our AI not being perfect, but we understand why it has some issues. Our dataset was limited, which is an issue Buolamwini repeatedly drove home. We spent a lot of time thinking about how to classify our data, settling on something objective like Buolamwini chose to. So many of Buolamwini’s concerns about AI resonated deeply with us as we worked on this project, and we hope that more experts make these considerations as they work with AI, because lack of these considerations can have massive consequences. 
            </p>
      <h2 class="sectionTitle">References</h2>
      <h3>Buolamwini, J. (2023). <i>Unmasking AI: My mission to protect what is human in a world of machines</i>. Random House.</h3>
      <h3><a href="https://github.com/luceiling/Project-2-LIS500.git" target="_blank">Github Repository</a></h3>
      </div> 
    </body>
</html>
        
